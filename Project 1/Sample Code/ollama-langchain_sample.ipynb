{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d75c5f",
   "metadata": {},
   "source": [
    "<h1> EXAMPLE: OLLAMA/LANGCHAIN </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625862c",
   "metadata": {},
   "source": [
    "https://github.com/jmorganca/ollama/blob/main/docs/tutorials/langchainpy.md\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff50c6",
   "metadata": {},
   "source": [
    "Use langchain to have an ollama model query a database of word embeddings\n",
    "\n",
    "You must have the ollama server running locally. If you are running a local machine, simply make sure the ollama app is running. If on the ARC (i.e., Open OnDemand), you will need to:\n",
    "\n",
    "<ul>\n",
    "    <li>Launch a new Terminal window and ssh into the your ARCC home:<br> <b>ssh username@arcc2.uc.edu</b></li>\n",
    "    <li>ssh into the compute node Jupyter is running on; this can be found under the \"My Interactive Sessions\" tab in Open OnDemand: <br><b>ssh compute-xx</b>\n",
    "    <li>Launch the ollama server: <br><b>ollama serve</b></li>\n",
    "    <li>Now, applications should be able to access the ollama server on localhost:11434 </li>\n",
    "</ul>\n",
    "<b>NOTES:</b>\n",
    "<ul>\n",
    "    <li>It's good practice to clone your base model to work off of: <br><b>ollama cp cource_model:tags new_model</b></li>\n",
    "    <li>It often takes a minute or more for the model to load each session - be patient</li>\n",
    "    <li>If the model isn't loading in to langchain, may need to ssh into the node with a separate Terminal and <br><b>ollama pull model_name:tags</b></li>\n",
    "    <li>Check the internet to make sure you are using the best prompt format for your model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14c135",
   "metadata": {},
   "source": [
    "<h3>Basic query for ollama & mistral</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b26dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' [INST] Instruction [/INST] Example model answer(s) [/INST] Follow-up instructions [/INST] '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde1ddd",
   "metadata": {},
   "source": [
    "<h1>Import packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import pprint as pp\n",
    "\n",
    "#also needs pip install: GPT4All, chromadb\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "#import text loaders\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "import langchain_community.vectorstores.utils as vutils\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "#import character splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#for storing embeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# for query\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435640c8",
   "metadata": {},
   "source": [
    "<h1> Simple query example:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540e0ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest')\n",
    "\n",
    "#query\n",
    "question = 'Why is the sky blue?'\n",
    "\n",
    "#example query, for testing\n",
    "response = ollama(''' [INST] Please be technical and answer in numbered paragraphs:{}\" \n",
    "        [/INST] JSON format: {{1:\"paragraph 1\",2:\"paragraph 2\",...}}\n",
    "        [/INST] If you don't know the answer, just say so [/INST] '''.format(question))\n",
    "\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7808145",
   "metadata": {},
   "source": [
    "<h1>Sentiment analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45021dad",
   "metadata": {},
   "source": [
    "<h3>On a list</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest',format='json')\n",
    "\n",
    "#define text for SA\n",
    "questions = [\n",
    "    'i love apples',\n",
    "    'i hate apples',\n",
    "    'i am indifferent about apples',\n",
    "    'apples are gross',\n",
    "    'apples are delicious',\n",
    "    'apples are yummy!',\n",
    "    'apples are blechh',\n",
    "    'apples are apples'\n",
    "]\n",
    "\n",
    "#output list\n",
    "output = []\n",
    "\n",
    "#for each text\n",
    "for q in questions:\n",
    "    \n",
    "    #get response in json format\n",
    "    response = ollama(''' [INST] What is the speakers sentiment towards the apples?\n",
    "    Please answer -1 for negative, 0 for neutral, and 1 for positive. Please return the json only:{} \n",
    "        [/INST]Use this format: {{\"sentence\":\"I like apples\",sentiment\":1,\"explanation\":\"the speaker seems to enjoy apples\"}}\n",
    "        [/INST]Do not follow up with any text[/INST] '''.format(q))\n",
    "    \n",
    "    #append to output list\n",
    "    output.append(response)\n",
    "\n",
    "    #pause briefly\n",
    "    time.sleep(.1)\n",
    "\n",
    "#pasrse output into json file\n",
    "output_str=''\n",
    "output_str = ','.join(output)\n",
    "output_str = '['+output_str+']'\n",
    "ouput_json = json.loads(output_str)\n",
    "\n",
    "#make dataframe\n",
    "pd.DataFrame.from_records(ouput_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4ac2f",
   "metadata": {},
   "source": [
    "<h3>...OR Concatenate list into a single string</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest',format='json')\n",
    "\n",
    "#define text for SA\n",
    "question_list = [\n",
    "    'i love apples',\n",
    "    'i hate apples',\n",
    "    'i am indifferent about apples',\n",
    "    'apples are gross',\n",
    "    'apples are delicious',\n",
    "    'apples are yummy!',\n",
    "    'apples are blechh',\n",
    "    'apples are apples'\n",
    "]\n",
    "\n",
    "#join into single delimted string\n",
    "question_string='; '.join(question_list)\n",
    "\n",
    "#generate response\n",
    "response = ollama(''' [INST] What is the speakers sentiment towards the apples in each list element?\n",
    "    Please answer -1 for negative, 0 for neutral, and 1 for positive. Please return the json only:{} \n",
    "        [/INST]The list is delimited by semicolons. Use this output format: {{\"sentence\":\"I like apples\",sentiment\":1,\"explanation\":\"the speaker seems to enjoy apples\"}}\n",
    "        [/INST]Do not follow up with any text[/INST] '''.format(question_string))\n",
    "\n",
    "pp.pprint(response)\n",
    "\n",
    "#format into dataframe\n",
    "pd.DataFrame.from_records(json.loads(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdef4e",
   "metadata": {},
   "source": [
    "<h1>#1: Load text from web source</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462850e1",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/document_loaders/web_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://americanliterature.com/author/benjamin-franklin/essay/the-morals-of-chess\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7077052",
   "metadata": {},
   "source": [
    "<h3>Split tokens into smaller chunks if needed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may need to tweak chunk size/overlap for better answer\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6081fb",
   "metadata": {},
   "source": [
    "<h3>Create word embeddings using ollama model and store in vector database (may take a while)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d342335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create embedding\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"mistral:latest\")\n",
    "\n",
    "# work with db in memory\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed)\n",
    "\n",
    "# save db to disk: https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "#vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# load db from disk\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d02af3",
   "metadata": {},
   "source": [
    "<h3>Run query</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38462db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may take some time on first load\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest')\n",
    "\n",
    "#queries database for similarities\n",
    "question=''' [INST] What is Benjamin Franklin talking about here? Please summarize in 3 paragraphs.[/INST] '''\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "#formats similarities into text response\n",
    "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever())\n",
    "pp.pprint(qachain({'query': question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e636c",
   "metadata": {},
   "source": [
    "<h1>#2: Multilple local files - map to template</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad77bf",
   "metadata": {},
   "source": [
    "<h3>Load file data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36a70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert csv to json\n",
    "def csv_to_json(csv_filepath):\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    json_data = df.to_json(orient='records')\n",
    "    return json_data\n",
    "\n",
    "CLINICAL_DATA_DIR = 'UC_hackathon_data_draft'\n",
    "\n",
    "#get local file list\n",
    "file_names = [f for f in os.listdir(CLINICAL_DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "#create list of imported clinical data and covert to DataFrame\n",
    "all_clinical_data_l=list() \n",
    "for file_name in file_names:\n",
    "    \n",
    "    clinical_file_path=os.path.join(CLINICAL_DATA_DIR,file_name)\n",
    "    clinical_json_str=csv_to_json(clinical_file_path)\n",
    "    clinical_json_dict=json.loads(clinical_json_str)[0]\n",
    "    all_clinical_data_l.append(clinical_json_dict)\n",
    "    \n",
    "df=pd.DataFrame.from_dict(all_clinical_data_l)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d890635",
   "metadata": {},
   "source": [
    "<h3>Create model and prompt</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd0ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#connect to llm model\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest')\n",
    "\n",
    "#take study titles and join into single delimited string\n",
    "just_titles_l= [study['Study Title'] for study in all_clinical_data_l]\n",
    "titles_string=\"; \".join(just_titles_l)\n",
    "\n",
    "map_template = ''' [INST] You are a factual clincal review assistant.\n",
    "    Based on the following list of clinical Study Titles please summarize the main medical condition being examined: \n",
    "    {clinical_data_list} [/INST] '''\n",
    "\n",
    "#assign template text to PromptTemplate object\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "#create llm chain\n",
    "map_chain = LLMChain(llm=ollama, prompt=map_prompt,verbose=False)\n",
    "\n",
    "#execute prompt based on template, mapping variables to input\n",
    "response=map_chain.run(clinical_data_list=titles_string)\n",
    "\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfb93b",
   "metadata": {},
   "source": [
    "<h1>#3: Multiple local files - load from pandas</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1636932",
   "metadata": {},
   "source": [
    "<h3>Load files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a13e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert csv to json\n",
    "def csv_to_json(csv_filepath):\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    json_data = df.to_json(orient='records')\n",
    "    return json_data\n",
    "\n",
    "CLINICAL_DATA_DIR = 'UC_hackathon_data_draft'\n",
    "\n",
    "#get local file list\n",
    "file_names = [f for f in os.listdir(CLINICAL_DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "#create list of imported clinical data and covert to DataFrame\n",
    "all_clinical_data_l=list() \n",
    "\n",
    "for file_name in file_names:\n",
    "    \n",
    "    clinical_file_path=os.path.join(CLINICAL_DATA_DIR,file_name)\n",
    "    clinical_json_str=csv_to_json(clinical_file_path)\n",
    "    clinical_json_dict=json.loads(clinical_json_str)[0]\n",
    "    \n",
    "    all_clinical_data_l.append(clinical_json_dict)\n",
    "    \n",
    "df=pd.DataFrame.from_dict(all_clinical_data_l)\n",
    "\n",
    "#for testing only load some of the documents\n",
    "loader = DataFrameLoader(df,page_content_column=\"Study Title\")\n",
    "docs = loader.load()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9e836",
   "metadata": {},
   "source": [
    "<h3>Create vector store database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbec5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))\n",
    "\n",
    "#create ebedding\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"mistral:latest\")\n",
    "\n",
    "#Filters out metadata types that are not supported for a vector store.\n",
    "docs = vutils.filter_complex_metadata(docs)\n",
    "\n",
    "# work with db in memory\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c71bae",
   "metadata": {},
   "source": [
    "<h3>Create model and prompt</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may take some time on first load\n",
    "ollama = Ollama(base_url='http://localhost:11434', model='mistral:latest')\n",
    "\n",
    "#queries database for similarities\n",
    "question = ''' [INST] You are a factual clincal review assistant.Based on the data Study Titles column, \n",
    "please summarize the main medical condition being examined[/INST] '''\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "#formats similarities into text response\n",
    "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever())\n",
    "pp.pprint(qachain({'query': question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bdc9e",
   "metadata": {},
   "source": [
    "<h1>#4: Load from local files</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a4ed2",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/document_loaders/csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gen_ai]",
   "language": "python",
   "name": "conda-env-.conda-gen_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
